{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5\n",
    "\n",
    "**Submission deadline: last lab session before or on Tuesday, 09.5.17**\n",
    "\n",
    "**Points: 9 + 10 bonus points**\n",
    "\n",
    "\n",
    "## Downloading this notebook\n",
    "\n",
    "This assignment is an Jupyter notebook. Download it by cloning https://github.com/janchorowski/nn_assignments. Follow the instructions in its README for instructions.\n",
    "\n",
    "For programming exerciese add your solutions to the notebook. For math exercies please provide us with answers on paper or type them in the notebook (it supports Latex-like equations).\n",
    "\n",
    "Please do not hesitate to use GitHubâ€™s pull requests to send us corrections!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular network implementation\n",
    "\n",
    "This assignment builds on code from Assignment 4, Problem 7. \n",
    "For your convenience, we have copied the code below. Please copy your solution from the old list, or fill in the blanks below to get a working network.\n",
    "\n",
    "In the following cells, I implement in a modular way a feedforward neural network. Please study the code - many network implementations follow a similar pattern.\n",
    "\n",
    "Please make sure that the network trains to nearly 100% accuracy on Iris.\n",
    "\n",
    "## Task\n",
    "\n",
    "Your job is to implement SGD training on MNIST with the following elements:\n",
    "1. SGD + momentum\n",
    "2. weight decay\n",
    "3. early stopping\n",
    "\n",
    "In overall, you should get below **2% testing errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return []\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        return []\n",
    "    \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, num_in, num_out, weight_init=None, bias_init=None, **kwargs):\n",
    "        super(AffineLayer, self).__init__(**kwargs)\n",
    "        if weight_init is None:\n",
    "            #\n",
    "            # DONE: propose a default initialization scheme.\n",
    "            # Type a sentence explaining why, and if you use a reference, \n",
    "            # cite it here\n",
    "            # http://cs231n.github.io/neural-networks-2/\n",
    "            # weight_init = Uniform(width=0.3) \n",
    "            weight_init = IsotropicGaussian(std=1.0/np.sqrt(num_in))\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        \n",
    "        self.W = weight_init.generate(self.rng, (num_out, num_in))\n",
    "        self.b = bias_init.generate(self.rng, (num_out, 1))\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W','b']\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        #Save X for later reusal\n",
    "        fprop_context = dict(X=X)\n",
    "        Y = np.dot(self.W, X) +  self.b\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        dLdX = self.W.T.dot(dLdY)  # DONE: fill in gradient computation\n",
    "        return dLdX\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        dLdW = np.dot(dLdY, X.T)\n",
    "        dLdb = dLdY.sum(1, keepdims=True)\n",
    "        return [dLdW, dLdb]\n",
    "    \n",
    "class TanhLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TanhLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.tanh(X)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        dLdX = dLdY * ((1 - Y ** 2))  # DONE: Fill in proper gradient computation\n",
    "        return dLdX\n",
    "\n",
    "    \n",
    "class ReLULayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReLULayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.maximum(X, 0.0)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        return dLdY * (Y>0)\n",
    "\n",
    "    \n",
    "class SoftMaxLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SoftMaxLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def compute_probabilities(self, X):\n",
    "        O = X - X.max(axis=0, keepdims=True)\n",
    "        O = np.exp(O)\n",
    "        O /= O.sum(axis=0, keepdims=True)\n",
    "        return O\n",
    "    \n",
    "    def fprop_cost(self, X, Y):\n",
    "        NS = X.shape[1]\n",
    "        O = self.compute_probabilities(X)\n",
    "        Cost = -1.0/NS * np.log(O[Y.ravel(), range(NS)]).sum()\n",
    "        return Cost, O, dict(O=O, X=X, Y=Y)\n",
    "    \n",
    "    def bprop_cost(self, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        Y = fprop_context['Y']\n",
    "        O = fprop_context['O']\n",
    "        NS = X.shape[1]\n",
    "        dLdX = O.copy()\n",
    "        dLdX[Y, range(NS)] -= 1.0\n",
    "        dLdX /= NS\n",
    "        return dLdX\n",
    "    \n",
    "class FeedForwardNet(object):\n",
    "    def __init__(self, layers=None):\n",
    "        if layers is None:\n",
    "            layers = []\n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "    \n",
    "    @parameters.setter\n",
    "    def parameters(self, values):\n",
    "        for ownP, newP in zip(self.parameters, values):\n",
    "            ownP[...] = newP\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        param_names = []\n",
    "        for layer in self.layers:\n",
    "            param_names += layer.parameter_names\n",
    "        return param_names\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "        return self.layers[-1].compute_probabilities(X)\n",
    "    \n",
    "    def get_cost_and_gradient(self, X, Y):\n",
    "        fp_contexts = []\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "            fp_contexts.append(fp_context)\n",
    "        \n",
    "        L, O, fp_context = self.layers[-1].fprop_cost(X, Y)\n",
    "        dLdX = self.layers[-1].bprop_cost(fp_context)\n",
    "        \n",
    "        dLdP = [] #gradient with respect to parameters\n",
    "        for i in range(len(self.layers)-1):\n",
    "            layer = self.layers[len(self.layers)-2-i]\n",
    "            fp_context = fp_contexts[len(self.layers)-2-i]\n",
    "            dLdP = layer.get_gradients(dLdX, fp_context) + dLdP\n",
    "            dLdX = layer.bprop(dLdX, fp_context)\n",
    "        return L, O, dLdP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#training algorithms. They change the network!\n",
    "def GD(net, X, Y, alpha=1e-4, max_iters=1000000, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Simple batch gradient descent\n",
    "    \"\"\"\n",
    "    old_L = np.inf\n",
    "    for i in range(max_iters):\n",
    "        L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "        if old_L < L:\n",
    "            print(\"Iter: %d, loss increased!!\" % (i,))\n",
    "        if (old_L - L)<tolerance:\n",
    "            print(\"Tolerance level reached exiting\")\n",
    "            break\n",
    "        if i % 1000 == 0:\n",
    "            err_rate = (O.argmax(0) != Y).mean()\n",
    "            print(\"At iteration %d, loss %f, train error rate %f%%\" % (i, L, err_rate*100))\n",
    "        for P, G in zip(net.parameters, gradients):\n",
    "            P -= alpha * G\n",
    "        old_L = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "IrisX = iris.data.T\n",
    "IrisX = (IrisX - IrisX.mean(axis=1, keepdims=True)) / IrisX.std(axis=1, keepdims=True)\n",
    "IrisY = iris.target.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, loss 1.080751, train error rate 34.000000%\n",
      "At iteration 1000, loss 0.052947, train error rate 2.000000%\n",
      "At iteration 2000, loss 0.043951, train error rate 2.000000%\n",
      "At iteration 3000, loss 0.041582, train error rate 1.333333%\n",
      "At iteration 4000, loss 0.040593, train error rate 1.333333%\n",
      "At iteration 5000, loss 0.040044, train error rate 1.333333%\n",
      "At iteration 6000, loss 0.039658, train error rate 1.333333%\n",
      "At iteration 7000, loss 0.039325, train error rate 1.333333%\n",
      "At iteration 8000, loss 0.039000, train error rate 1.333333%\n",
      "At iteration 9000, loss 0.038663, train error rate 1.333333%\n",
      "At iteration 10000, loss 0.038315, train error rate 1.333333%\n",
      "At iteration 11000, loss 0.037962, train error rate 1.333333%\n",
      "At iteration 12000, loss 0.037618, train error rate 1.333333%\n",
      "At iteration 13000, loss 0.037295, train error rate 1.333333%\n",
      "At iteration 14000, loss 0.037000, train error rate 1.333333%\n",
      "At iteration 15000, loss 0.036738, train error rate 1.333333%\n",
      "At iteration 16000, loss 0.036505, train error rate 1.333333%\n",
      "At iteration 17000, loss 0.036299, train error rate 1.333333%\n",
      "At iteration 18000, loss 0.036113, train error rate 1.333333%\n",
      "At iteration 19000, loss 0.035941, train error rate 1.333333%\n",
      "At iteration 20000, loss 0.035775, train error rate 1.333333%\n",
      "At iteration 21000, loss 0.035608, train error rate 1.333333%\n",
      "At iteration 22000, loss 0.035433, train error rate 1.333333%\n",
      "At iteration 23000, loss 0.035238, train error rate 1.333333%\n",
      "At iteration 24000, loss 0.035004, train error rate 1.333333%\n",
      "At iteration 25000, loss 0.034700, train error rate 1.333333%\n",
      "At iteration 26000, loss 0.034307, train error rate 1.333333%\n",
      "At iteration 27000, loss 0.033856, train error rate 1.333333%\n",
      "At iteration 28000, loss 0.033377, train error rate 1.333333%\n",
      "At iteration 29000, loss 0.032874, train error rate 1.333333%\n",
      "At iteration 30000, loss 0.032355, train error rate 1.333333%\n",
      "At iteration 31000, loss 0.031832, train error rate 1.333333%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-82ce28bb1b33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mSoftMaxLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         ])\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIrisX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIrisY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-572c19545058>\u001b[0m in \u001b[0;36mGD\u001b[0;34m(net, X, Y, alpha, max_iters, tolerance)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mold_L\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cost_and_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mold_L\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iter: %d, loss increased!!\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-28b5b188f0a8>\u001b[0m in \u001b[0;36mget_cost_and_gradient\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mdLdX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mdLdP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#gradient with respect to parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-28b5b188f0a8>\u001b[0m in \u001b[0;36mbprop_cost\u001b[0;34m(self, fprop_context)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mNS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mdLdX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mdLdX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mdLdX\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mNS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdLdX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "# Here we verify that the network can be trained on Irises.\n",
    "# Most runs should result in 100% accurracy\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(4,10),\n",
    "        TanhLayer(),\n",
    "        AffineLayer(10,3),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "GD(net, IrisX, IrisY, 1e-1, tolerance=1e-7, max_iters=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from Fuel\n",
    "\n",
    "The following cell prepares the data pipeline in fuel. please see SGD template for usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "MNIST.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}), \n",
    "    (Flatten, [], {'which_sources': 'features'}),\n",
    "    (Mapping, [lambda batch: (b.T for b in batch)], {}) )\n",
    "\n",
    "mnist_train = MNIST((\"train\",), subset=slice(None,50000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "mnist_train_stream = DataStream.default_stream(\n",
    "    mnist_train,\n",
    "    iteration_scheme=ShuffledScheme(mnist_train.num_examples, 100))\n",
    "                                               \n",
    "mnist_validation = MNIST((\"train\",), subset=slice(50000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "mnist_validation_stream = DataStream.default_stream(\n",
    "    mnist_validation, iteration_scheme=SequentialScheme(mnist_validation.num_examples, 250))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_test_stream = DataStream.default_stream(\n",
    "    mnist_test, iteration_scheme=SequentialScheme(mnist_test.num_examples, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing ('features', 'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (784, 100) containing float32\n",
      " - an array of size (1, 100) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (784, 250) containing float32\n",
      " - an array of size (1, 250) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print(\"The streams return batches containing %s\" % (mnist_train_stream.sources,))\n",
    "\n",
    "print(\"Each trainin batch consits of a tuple containing:\")\n",
    "for element in next(mnist_train_stream.get_epoch_iterator()):\n",
    "    print(\" - an array of size %s containing %s\" % (element.shape, element.dtype))\n",
    "    \n",
    "print(\"Validation/test batches consits of tuples containing:\")\n",
    "for element in next(mnist_test_stream.get_epoch_iterator()):\n",
    "    print(\" - an array of size %s containing %s\" % (element.shape, element.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 [4p]\n",
    "\n",
    "Implement the following additions to the SGD code below:\n",
    "1. Momentum [2p]\n",
    "2. Learning rate schedule [1p]\n",
    "3. Weight decay [1p]. One way to implement it is to use the functions `net.params` and `net.param_names` to get all parameters whose names are \"W\" and not \"b\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Please note, the code blow is able to train a SoftMax regression model on mnist to poor results (ca 8%test error), \n",
    "# you must improve it\n",
    "#\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def compute_error_rate(net, stream):\n",
    "    num_errs = 0.0\n",
    "    num_examples = 0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        O = net.fprop(X)\n",
    "        num_errs += (O.argmax(0) != Y).sum()\n",
    "        num_examples += X.shape[1]\n",
    "    return num_errs/num_examples\n",
    "\n",
    "def SGD(net, train_stream, validation_stream, test_stream):\n",
    "    i=0\n",
    "    e=0\n",
    "    \n",
    "    # DONE: initialize momentum variables\n",
    "    velocities = [np.zeros(P.shape) for P in net.parameters]\n",
    "    \n",
    "    best_valid_error_rate = np.inf\n",
    "    best_params = deepcopy(net.parameters)\n",
    "    best_params_epoch = 0\n",
    "    \n",
    "    train_erros = []\n",
    "    train_loss = []\n",
    "    validation_errors = []\n",
    "    \n",
    "    number_of_epochs = 5\n",
    "    patience_expansion = 1.5\n",
    "    \n",
    "    startA = 70.0\n",
    "    startB = 10000.0\n",
    "    \n",
    "    try:\n",
    "        while e < number_of_epochs: #This loop goes over epochs\n",
    "            e += 1\n",
    "            \n",
    "            #First train on all data from this batch\n",
    "            for X, Y in train_stream.get_epoch_iterator(): \n",
    "                i += 1\n",
    "                \n",
    "                L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "                err_rate = (O.argmax(0) != Y).mean()\n",
    "                \n",
    "                train_loss.append((i,L))\n",
    "                train_erros.append((i,err_rate))\n",
    "                \n",
    "                if i % 100 == 0:\n",
    "                    print(\"At minibatch %d, batch loss %f, batch error rate %f%%\" % (i, L, err_rate*100))\n",
    "                \n",
    "                for P, V, G, N in zip(net.parameters, velocities, gradients, net.parameter_names):\n",
    "                    if N=='W':\n",
    "                        G += P * 0.0001               # DONE: implement the weight decay addition to gradient\n",
    "                        \n",
    "                    alpha = startA / (startB + i)     # DONE: set a learning rate\n",
    "                    epsilon = 0.995                   # DONE: set the momentum constant \n",
    "                    V[...] = V * epsilon + alpha * G  # DONE: implement velocity update in momentum\n",
    "                    \n",
    "                    # DONE: set a more sensible learning rule here,\n",
    "                    # using your learning rate schedule and momentum\n",
    "                    P -= V\n",
    "                    \n",
    "            # After an epoch compute validation error\n",
    "            val_error_rate = compute_error_rate(net, validation_stream)\n",
    "            if val_error_rate < best_valid_error_rate:\n",
    "                number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "                best_valid_error_rate = val_error_rate\n",
    "                best_params = deepcopy(net.parameters)\n",
    "                best_params_epoch = e\n",
    "                validation_errors.append((i,val_error_rate))\n",
    "            print(\"After epoch %d: valid_err_rate: %f%% currently going to do %d epochs\" %(\n",
    "                e, val_error_rate, number_of_epochs))\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Setting network parameters from after epoch %d\" %(best_params_epoch))\n",
    "        net.parameters = best_params\n",
    "        \n",
    "        subplot(2,1,1)\n",
    "        train_loss = np.array(train_loss)\n",
    "        semilogy(train_loss[:,0], train_loss[:,1], label='batch train loss')\n",
    "        legend()\n",
    "        \n",
    "        subplot(2,1,2)\n",
    "        train_erros = np.array(train_erros)\n",
    "        plot(train_erros[:,0], train_erros[:,1], label='batch train error rate')\n",
    "        validation_errors = np.array(validation_errors)\n",
    "        plot(validation_errors[:,0], validation_errors[:,1], label='validation error rate', color='r')\n",
    "        ylim(0,0.2)\n",
    "        legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 [5p]\n",
    "\n",
    "Tune the following network to reach below 1.9% error rate on\n",
    "the validation set. This should result in a test error below 2%. To\n",
    "tune the network you will need to:\n",
    "1. choose the number of layers (more than 1, less than 5),\n",
    "2. choose the number of neurons in each layer (more than 100,\n",
    "    less than 5000),\n",
    "3. pick proper weight initialization,\n",
    "4. pick proper learning rate schedule (need to decay over time,\n",
    "    good range to check on MNIST is about 1e-2 ... 1e-1 at the beginning and\n",
    "    half of that after 10000 batches),\n",
    "5. pick a momentum constant (probably a constant one will be OK).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 0.799708, batch error rate 15.000000%\n",
      "At minibatch 200, batch loss 0.605486, batch error rate 14.000000%\n",
      "At minibatch 300, batch loss 0.250772, batch error rate 8.000000%\n",
      "At minibatch 400, batch loss 0.496102, batch error rate 13.000000%\n",
      "At minibatch 500, batch loss 0.253951, batch error rate 7.000000%\n",
      "After epoch 1: valid_err_rate: 0.087300% currently going to do 3 epochs\n",
      "At minibatch 600, batch loss 0.304128, batch error rate 5.000000%\n",
      "At minibatch 700, batch loss 0.334409, batch error rate 8.000000%\n",
      "At minibatch 800, batch loss 0.454562, batch error rate 11.000000%\n",
      "At minibatch 900, batch loss 0.222301, batch error rate 7.000000%\n",
      "At minibatch 1000, batch loss 0.254828, batch error rate 8.000000%\n",
      "After epoch 2: valid_err_rate: 0.076700% currently going to do 4 epochs\n",
      "At minibatch 1100, batch loss 0.134974, batch error rate 4.000000%\n",
      "Setting network parameters from after epoch 2\n",
      "Test error rate: 0.085500\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f15adf37cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# DONE: pick a network architecture here. The one below is just \n",
    "# softmax regression\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(784, 1500, weight_init=Uniform(width=0.02)),\n",
    "        ReLULayer(),\n",
    "        AffineLayer(1500, 10),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "SGD(net, mnist_train_stream, mnist_validation_stream, mnist_test_stream)\n",
    "\n",
    "print(\"Test error rate: %f\" % (compute_error_rate(net, mnist_test_stream), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1147"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_error_rate(net, mnist_test_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem 3 [2p bonus]\n",
    "\n",
    "Implement norm constraints, i.e. limit the total\n",
    "norm of connections incoming to a neuron. In our case, this\n",
    "corresponds to clipping the norm of *rows* of weight\n",
    "matrices. An easy way of implementing it is to make a gradient\n",
    "step, then look at the norm of rows and scale down those that are\n",
    "over the threshold (this technique is called \"projected gradient descent\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 [2p bonus]\n",
    "\n",
    "Implement a **dropout** layer and try to train a\n",
    "network getting below 1.5% test error rates with dropout (the best\n",
    "result is below 1\\% for dropout!). Details: http://arxiv.org/pdf/1207.0580.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DropoutLayer(Layer):\n",
    "    def __init__(self, drop=0.25, **kwargs):\n",
    "        super(DropoutLayer, self).__init__(**kwargs)\n",
    "        self.lastD = None\n",
    "        self.drop=drop\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        scale = 1.0 / (1 - self.drop)\n",
    "        self.lastD = np.random.binomial([np.ones_like(X)], 1 - self.drop)[0] * scale\n",
    "        Y = X * self.lastD\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        return dLdY * self.lastD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 0.624029, batch error rate 16.000000%\n",
      "At minibatch 200, batch loss 0.588800, batch error rate 19.000000%\n",
      "At minibatch 300, batch loss 0.655500, batch error rate 16.000000%\n",
      "At minibatch 400, batch loss 0.483796, batch error rate 15.000000%\n",
      "At minibatch 500, batch loss 0.487553, batch error rate 11.000000%\n",
      "After epoch 1: valid_err_rate: 0.176300% currently going to do 3 epochs\n",
      "At minibatch 600, batch loss 0.894615, batch error rate 26.000000%\n",
      "At minibatch 700, batch loss 0.682865, batch error rate 22.000000%\n",
      "At minibatch 800, batch loss 0.721212, batch error rate 23.000000%\n",
      "At minibatch 900, batch loss 0.653542, batch error rate 23.000000%\n",
      "At minibatch 1000, batch loss 0.644121, batch error rate 25.000000%\n",
      "After epoch 2: valid_err_rate: 0.226600% currently going to do 3 epochs\n",
      "At minibatch 1100, batch loss 0.826860, batch error rate 22.000000%\n",
      "At minibatch 1200, batch loss 0.728111, batch error rate 23.000000%\n",
      "At minibatch 1300, batch loss 1.143699, batch error rate 40.000000%\n",
      "At minibatch 1400, batch loss 0.711872, batch error rate 23.000000%\n",
      "At minibatch 1500, batch loss 0.924882, batch error rate 30.000000%\n",
      "After epoch 3: valid_err_rate: 0.240500% currently going to do 3 epochs\n",
      "Test error rate: 0.255100\n"
     ]
    }
   ],
   "source": [
    "net = FeedForwardNet([\n",
    "        AffineLayer(784, 200, weight_init=Uniform(width=0.05)),\n",
    "        DropoutLayer(),\n",
    "        ReLULayer(),\n",
    "        AffineLayer(200, 10),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "\n",
    "SGD(net, mnist_train_stream, mnist_validation_stream, mnist_test_stream)\n",
    "print(\"Test error rate: %f\" % (compute_error_rate(net, mnist_test_stream), ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5 [3p bonus]\n",
    "\n",
    "Implement convolutional and max-pooling layers and (without dropout) get a test error rate below 1.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ConvLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        TODO\n",
    "        Y = np.maximum(X, 0.0)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        return dLdY * (Y>0)\n",
    "    \n",
    "class MaxPoolingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MaxPoolingLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        TODO\n",
    "        Y = np.maximum(X, 0.0)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        return dLdY * (Y>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6 [1-3p bonus]\n",
    "\n",
    "Implement a data augmentation method (e.g. rotations, noise, crops) that will yield a significant test error rate reduction for your network. Number of bonus points depends on the ingenuity of your solution and error rate gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
